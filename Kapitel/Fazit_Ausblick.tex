\section{Fazit und Ausblick}
\label{sec:fazit}

\subsection{Fazit}
Das Projektseminar Echtzeitsysteme 2018/2019 hat dem Team einen guten Einblick in die Probleml\"osung von echtzeitkritischen Aufgaben gegeben. Dabei hervorzuheben ist, dass die freie Gestaltung der Aufgabenstellung es erm\"oglicht hat, viele L\"osungsm\"oglichkeiten auszuprobieren und kreativ zu sein. Die Herausforderung, auf einem ressourcenbeschr\"ankten Roboterauto eine robuste, autonome Fahrassistenz zu betreiben, hat gezeigt, welche allt\"aglichen Probleme in der Industrie auftreten k\"onnen. Bei allen Ideen zur Verbesserung von Prozessen muss daran gedacht werden, die eventuell auftretenden Zeitverz\"ogerungen im System zu beachten.

Des Weiteren hat das Team einen tiefen Einblick sowohl in die unterschiedlichsten Methoden der Bildverarbeitung mittels OpenCV, als auch in die Regelung von autonomen Fahrzeugen und den Einsatz eines neuronalen Netzwerkes f\"ur die Schildererkennung erhalten. Die Arbeit mit dem Software-Framework ROS\cite{ROS} hat es erm\"oglicht, die Kommunikation zwischen den einzelnen Komponenten des Projekts gut zu strukturieren.

Durch Nutzung dieser Methoden und Technologien ist es uns zu Ende des Projekts gelungen,
eine funktionierende Linienerkennung, eine entsprechende Regelung
auf erkannte Linien zur autonomen Bew\"altigung des Rundkurses sowie eine Schildererkennung
mittels eines neuronalen Netzwerks zu implementieren.

\subsection{Probleme}
W\"ahrend des Projekts sind einige Probleme wiederholt aufgetaucht. Das H\"aufigste war das Ausfallen der \textbf{\textit{uc\underline{\ }bridge}}, was die Arbeit mit dem Roboterauto oft schwer beeintr\"achtigt hat. Die Ultraschallsensoren, insbesondere der Vordere, waren oft stark verrauscht oder haben komplett ausgesetzt. Zuletzt haben der Antrieb und das Differential des vorderen linken Rades nicht mehr funktioniert, wodurch das Auto oft aus der Fahrbahn gezogen wurde.

Manchmal, wenn das Roboterauto mit einem Monitor verbunden werden sollte, hat der VGA-Anschluss des Autos kein Signal gesendet und man musste das Roboterauto manuell neu starten, um einen Monitor anschlie\ss{}en zu k\"onnen. 

Die Lichtverh\"altnisse in den R\"aumen haben die Funktionalit\"at der Bildverarbeitung au\ss erdem stark beeintr\"achtigt: Je nach Tageszeit hat das Testen zu unterschiedlichen Ergebnissen gef\"uhrt. Das Team hat am Anfang des Projekts beschlossen, die bereitgestellte Webcam statt der Kinect zu nutzen, doch ohne eine feste Halterung hat sich die Kamera h\"aufig verstellt und musste neu eingestellt werden. Eine vorgegebene Halterung w\"are daher w\"unschenswert gewesen.

\subsection{Ausblick}
Obwohl die grunds\"atzlich angestrebte Funktionalit\"at erreicht wurde, bieten einzelne
Aspekte des Projekts noch Raum f\"ur zuk\"unftige Verbesserungen und Optimierungen.

Die Linienerkennung erfolgt zur Zeit \"uber eine direkte Transformation des aufgenommenen
Webcam-Bilds in die Vogelperspektive, wobei alle weiteren Verarbeitungsschritte auf dem
transformierten Bild basieren.\\
Um sich den relativ zeitaufwendigen Schritt der perspektivischen Transformation zu sparen, k\"onnte
in Zukunft eine Unterklasse von \texttt{LaneDetector} implementiert werden, die
nur einzelne Punkte transformiert und direkt auf dem Quellbild arbeitet. Die daf\"ur
ben\"otigten Transformationsmatrizen sind bereits vorhanden.
%TODO: Ausblick Regelung? Anderen Ansatz verwenden/testen? (Frederic)

Die Schildererkennung hat bereits sehr gute Ergebnisse geliefert, oft auf weite Entfernungen, doch das Roboterauto reagiert momentan noch nicht auf erkannte Schilder.
Ein Zustandsautomat zur Weiterverarbeitung der erkannten Objekte wurde bereits implementiert, dessen
Ausgabe-Messages m\"ussten im n\"achsten Schritt jedoch noch von der Fahrzeugsteuerung
abonniert und weiterverwendet werden.